{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "#!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q evaluate jsonlines rouge_score bert-score\n",
        "!pip install transformers peft accelerate bitsandbytes jsonlines\n",
        "!pip install evaluate gradio\n",
        "import evaluate\n"
      ],
      "metadata": {
        "id": "TLKhXjurR0CH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "max_seq_length = 2048  # Choose any! We auto support ROPE scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, bFloat16 for Ampere+\n",
        "\n",
        "model_name_or_path = \"jacopoda/lora_model\"\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name_or_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        "    # token = \"hf_...\", #se il nostro modello non Ã¨ public\n",
        "    # Use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n"
      ],
      "metadata": {
        "id": "2jO4kp7KRz-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288a5218-306d-426c-e1c2-23671e37f36b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Mock data for demonstration purposes\n",
        "current_question = {\"question\": \"\", \"answer\": \"\"}\n",
        "\n",
        "def generate_question(argument):\n",
        "    \"\"\"Generates a question and answer based on the provided argument.\"\"\"\n",
        "    # Mock question generation logic for simplicity\n",
        "    question = f\"What is an important aspect of {argument}?\"\n",
        "    answer = f\"{argument} is important because it demonstrates the ability to apply knowledge to practical scenarios.\"\n",
        "    current_question[\"question\"] = question\n",
        "    current_question[\"answer\"] = answer\n",
        "    return question\n",
        "\n",
        "def evaluate_answer(user_answer):\n",
        "    \"\"\"Evaluates the user's answer without showing the correct answer.\"\"\"\n",
        "    feedback = \"Your answer has been submitted! Click 'Reveal Answer' to see the correct answer.\"\n",
        "    return feedback\n",
        "\n",
        "def reveal_answer():\n",
        "    \"\"\"Reveals the correct answer.\"\"\"\n",
        "    return f\"Correct Answer: {current_question['answer']}\"\n",
        "\n",
        "def reset_fields():\n",
        "    \"\"\"Resets the question, answer, and feedback fields for a new topic.\"\"\"\n",
        "    current_question[\"question\"] = \"\"\n",
        "    current_question[\"answer\"] = \"\"\n",
        "    return \"\", \"\", \"\", \"\"\n",
        "\n",
        "# Set up the Gradio interface\n",
        "with gr.Blocks() as interactive_quiz:\n",
        "    gr.Markdown(\"## ðŸ§  Interactive Quiz\")\n",
        "    gr.Markdown(\"Provide a topic, answer a generated question, and reveal the correct answer!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        argument_input = gr.Textbox(label=\"Topic or Argument\", placeholder=\"E.g., Machine Learning\")\n",
        "        generate_btn = gr.Button(\"Generate Question\")\n",
        "        change_topic_btn = gr.Button(\"Change Topic\")\n",
        "\n",
        "    question_display = gr.Textbox(label=\"Generated Question\", interactive=False)\n",
        "    user_answer_input = gr.Textbox(label=\"Your Answer\", placeholder=\"Type your answer here...\")\n",
        "    evaluate_btn = gr.Button(\"Submit Answer\")\n",
        "    reveal_answer_btn = gr.Button(\"Reveal Answer\")\n",
        "    feedback_output = gr.HTML(label=\"Feedback\")\n",
        "    answer_output = gr.HTML(label=\"Correct Answer\")\n",
        "\n",
        "    # Define interactions\n",
        "    generate_btn.click(\n",
        "        generate_question,\n",
        "        inputs=argument_input,\n",
        "        outputs=question_display\n",
        "    )\n",
        "\n",
        "    evaluate_btn.click(\n",
        "        evaluate_answer,\n",
        "        inputs=user_answer_input,\n",
        "        outputs=feedback_output\n",
        "    )\n",
        "\n",
        "    reveal_answer_btn.click(\n",
        "        reveal_answer,\n",
        "        inputs=[],\n",
        "        outputs=answer_output\n",
        "    )\n",
        "\n",
        "    # Reset fields when changing the topic\n",
        "    change_topic_btn.click(\n",
        "        reset_fields,\n",
        "        inputs=[],\n",
        "        outputs=[question_display, user_answer_input, feedback_output, answer_output]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interactive_quiz.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "DFygRMPpz4T6",
        "outputId": "33806a13-c7af-48b7-fcfa-959cc132eb7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c462ef76e3c433c9b6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c462ef76e3c433c9b6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c462ef76e3c433c9b6.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}